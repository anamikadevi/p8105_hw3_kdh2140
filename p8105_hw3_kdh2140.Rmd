---
title: "Data Science Homework 3"
author: Kristina Howell
output: github_document
---

The following settings and libraries will be used throughout the homework. 

```{r settings, message = FALSE}
library(tidyverse)
library(ggplot2)
library(ggridges)
library(patchwork)
library(hexbin)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis", 
  ggplot2.continuous.fill = "virirdis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load the datasets from the p8105 library. 

```{r dataset}
library(p8105.datasets)
data("instacart")
```

#### Dataset Description

The dataset **instacart** contains information about specific orders from the instacart online grocery service with 1,384,617 observations from 131,209 unique users. Key variables in the dataset include _department_, _aisle_, _product_name_, and _order_id_, as well as various entries representing time, day of week, and days since prior order. 

Overall, the dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Each row represents an individual item ordered from a user. For example, the first 8 rows represent:

* the _order_id_ 1 
* from _user_id_ 112108 
* includes items from _department_ "dairy eggs", "produce", and "canned goods"
* featuring _product_name_ "Bulgarian Yogurt" ( _product_id_: 49302 ) and _product_name_ "Organic Celery Hearts" ( _product_id_: 10246 )

The **complete list of variables** in the dataset is as follows: `r names(instacart)`.

#### Problem 1 Questions

##### How many aisles are there, and which aisles are the most items ordered from?

The first code chunk identifies the total number of aisles: 134.
```{r aisle_number}
instacart %>% 
  summarize(n_aisle = n_distinct(aisle_id))
```

The second code chunk creates a tibble listing the aisles and how much they're ordered from. 
```{r aisle_orders}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_ordered = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_ordered))) 
```

_Live lecture code_

```{r code_better}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

##### Plot the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

Continuing from the above code chunk, 

_Live lecture code_

```{r aisle_plot}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  filter(n >= 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n, color = aisle)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


##### Table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

_Live lecture code_

```{r aisle_rank}
instacart %>% 
  filter(aisle %in% c( "baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

#####  Table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

_Live lecture code_

```{r item_comparison}
instacart %>% 
  filter(product_name %in% c( "Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2

#### Load in the dataset and tidy 

The data is imported through the read_csv function from the tidyverse library. The data cleaning steps include cleaning the names, through the clean_names() function in the janitor library, and reorganizing the data to represent activity as a variable, using the pivot_longer() function. The time_minute variable is then mutated into a numeric variable. All variables are numeric, except for day and week_day.

```{r load_data, message = FALSE}
accel = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "time_minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
   mutate(
    week_day = case_when(
      day == "Monday" ~ "weekday", 
      day == "Tuesday" ~ "weekday", 
      day == "Wednesday" ~ "weekday", 
      day == "Thursday" ~ "weekday", 
      day == "Friday" ~ "weekday", 
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      TRUE ~ ""
    )) %>% 
  mutate(time_minute = as.numeric(time_minute))
```

#### Dataset Description

The dataset **accel** contains activity collected from a 63 year old male through an accelerometer. This data was collected due to an admission to the Advanced Cardiac Care Center of Columbia University Medical Center and subsequent diagnosis of congestive heart failure (CHF). 

The activity data ( _activity_count_ ) is recorded by minute ( _time_minute_ ). These are then divided by day ( _day_id_ ) with the specific day of the week described in the _day_ variable. The dataset ranges from 1 - 5 weeks, as described in the _week_ variable. An additional variable, _week_day_, was created as a binary character, denoting whether a particular day observation was a weekday or weekend. Weekday is defined as Monday through Friday and weekend is defined as Saturday and Sunday.

Overall, the dataset contains `r nrow(accel)` rows and `r ncol(accel)` columns. Each row represents an individual minute of each day. 


#### Traditional Analyses

The following code chunk creates a table that describes total activity per day, created using the mutate function to sum the activity_count (recorded by minute) when grouped by day. The table is then formatted from long to wide, using the pivot_wider function, to increase readability. 

```{r activity_table, message = FALSE}
accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  relocate("week", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday") %>% 
  knitr::kable(digits = 1)
```

Trends identified by viewing the table include:

* The individual has less activity on Saturday as the weeks go on.
* Tuesday, Wednesday, and Thursday levels of activity remain steady throughout the 5 weeks, while Friday, Saturday, Sunday, and Monday levels of activity vary greatly throughout the 5 weeks. 

#### Plot activity data by minute and identified by day of week
 
```{r activity_plot, message = FALSE}
accel %>% 
  group_by(day, week) %>% 
  ggplot(aes(x = time_minute, y = activity_count, color = day)) +
  geom_line()
##Need to add labels
```

I believe this is the plot you want. It is not very readable.


```{r activity_plot_nice, message = FALSE}
accel %>% 
  group_by(day, week) %>% 
  ggplot(aes(x = time_minute, y = activity_count, color = day)) +
  geom_smooth()
```

This is not the plot you want but i think it looks nice. 

## Problem 3

Load in the dataset and tidy. 

```{r}
library(p8105.datasets)
data("ny_noaa")
```

#### Dataset Description

The dataset **ny_noaa** was pulled from the NOAA National Climatic Data Center on August 15, 2017. It includes information from all weather stations in New York state ranging from January 1, 1981 to December 31, 2010. 

Weather stations are identified by the variable _id_ and the date is recorded in the variable _date_. Informational variables include _pcrp_ (), _snow_ (), _snwd_ (), _tmax_ (), and _tmin_ ().

Overall, the dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Each column represents an individual date per weather station.

#### Problem 3 Questions

##### Data cleaning 

The data cleaning step includes creating separate variables for year, month, and day through the separate() function. Temperature variables are mutated from character to numeric variables.

The temperature variable is then reformatted to display temperature in degrees celsius, rather than tenths of a degree celsius. Precipitation is also reformatted to display rainfall in mm, rather than tenths of a mm. Snowfall and snowdepth are already displayed in mm. 

```{r}
ny_noaa_df = 
ny_noaa %>% 
  separate(date, into = c("year", "month"), sep = 5) %>% 
  separate(year, into = c("year", "x"), sep = 4) %>% 
  separate(month, into = c("month", "day"), sep = 2) %>% 
  separate(day, into = c("y", "day"), sep = 1) %>% 
  select(-x, -y) %>% 
  #note: when separating date into different variables in a tidier way, r crashes, most likely due to the size of the dataset. 
  mutate(tmin = as.numeric(tmin),
         tmax = as.numeric(tmax),
         month = as.numeric(month)
         ) %>% 
  mutate(
    tmin = tmin / 10,
    tmax = tmax / 10, 
    prcp = prcp / 10
    )

ny_noaa_df %>% 
  count(snow) %>% 
  arrange(desc(n))
```
    
For snowfall, the most commonly observed value is 0. This is most likely due to the fact that, the majority of the year, it is not snowing in NYS.
    
##### Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_df %>% 
  filter(month == c(01, 06)) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) + 
  geom_point() +
  geom_line() +
  theme(legend.position = "none") +
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

  # Note: legend
```


##### Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
  ny_noaa_df %>% 
  drop_na(tmax, tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex()

## need to change to geom_hex
```



```{r}
ny_noaa_df %>% 
  ggplot(aes(x = as.numeric(tmax), y = as.numeric(tmin))) + 
  geom_hex()
```



```{r}
snow_plot = 
  ny_noaa_df %>% 
  filter(snow > 0) %>% 
  filter(snow < 100) %>% 
  ggplot(aes(x = snow, y = as.factor(year))) +
  geom_density_ridges()
```









