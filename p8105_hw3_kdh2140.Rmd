---
title: "Data Science Homework 3"
author: Kristina Howell
output: github_document
---

The following settings and libraries will be used throughout the homework. 

```{r settings, message = FALSE}
library(tidyverse)
library(ggplot2)
library(ggridges)
library(patchwork)
library(hexbin)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis", 
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load the datasets from the p8105 library. 

```{r dataset}
library(p8105.datasets)
data("instacart")
```

#### Dataset Description

The dataset **instacart** contains information about specific orders from the instacart online grocery service with 1,384,617 observations from 131,209 unique users. Key variables in the dataset include _department_, _aisle_, _product_name_, and _order_id_, as well as various entries representing time, day of week, and days since prior order. 

Overall, the dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Each row represents an individual item ordered from a user. For example, the first 8 rows represent:

* the _order_id_ 1 
* from _user_id_ 112108 
* includes items from _department_ "dairy eggs", "produce", and "canned goods"
* featuring _product_name_ "Bulgarian Yogurt" ( _product_id_: 49302 ) and _product_name_ "Organic Celery Hearts" ( _product_id_: 10246 )

The **complete list of variables** in the dataset is as follows: `r names(instacart)`.

#### Problem 1 Questions

##### How many aisles are there, and which aisles are the most items ordered from?

The first code chunk identifies the total number of aisles: 134.
```{r aisle_number}
instacart %>% 
  summarize(n_aisle = n_distinct(aisle_id))
```

The second code chunk creates a tibble listing the aisles and how much they're ordered from. 
```{r aisle_orders}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_ordered = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_ordered))) 
```

_Live lecture code_

```{r code_better}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

##### Plot the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

Continuing from the above code chunk, 

_Live lecture code_

```{r aisle_plot}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  filter(n >= 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n, color = aisle)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


##### Table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

_Live lecture code_

```{r aisle_rank}
instacart %>% 
  filter(aisle %in% c( "baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

#####  Table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

_Live lecture code_

```{r item_comparison}
instacart %>% 
  filter(product_name %in% c( "Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2

#### Load in the dataset and tidy 

The data is imported through the read_csv function from the tidyverse library. The data cleaning steps include cleaning the names, through the clean_names() function in the janitor library, and reorganizing the data to represent activity as a variable, using the pivot_longer() function. The time_minute variable is then mutated into a numeric variable. Therefore, all variables are numeric, except for day and week_day.

```{r load_data, message = FALSE}
accel = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "time_minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
   mutate(
    week_day = case_when(
      day == "Monday" ~ "weekday", 
      day == "Tuesday" ~ "weekday", 
      day == "Wednesday" ~ "weekday", 
      day == "Thursday" ~ "weekday", 
      day == "Friday" ~ "weekday", 
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      TRUE ~ ""
    )) %>% 
  mutate(time_minute = as.numeric(time_minute))
```

#### Dataset Description

The dataset **accel** contains activity collected from a 63 year old male through an accelerometer. This data was collected due to an admission to the Advanced Cardiac Care Center of Columbia University Medical Center and subsequent diagnosis of congestive heart failure (CHF). 

The activity data ( _activity_count_ ) is recorded by minute ( _time_minute_ ). These are then divided by day ( _day_id_ ) with the specific day of the week described in the _day_ variable. The dataset ranges from 1 - 5 weeks, as described in the _week_ variable. An additional variable, _week_day_, was created as a binary character, denoting whether a particular day observation was a weekday or weekend. Weekday is defined as Monday through Friday and weekend is defined as Saturday and Sunday.

Overall, the dataset contains `r nrow(accel)` rows and `r ncol(accel)` columns. Each row represents an individual minute of each day throughout the five weeks. 


#### Traditional Analyses

The following code chunk creates a table that describes total activity per day, created using the mutate function to sum the activity_count (recorded by minute) when grouped by day. The table is then formatted from long to wide, using the pivot_wider function, to increase readability. 

```{r activity_table, message = FALSE}
accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  relocate("week", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday") %>% 
  knitr::kable(digits = 1)
```

Trends identified by viewing the table include:

* The individual has less activity on Saturday as the weeks go on. This may be an error in the recording of the data. 
* Tuesday, Wednesday, and Thursday levels of activity remain steady throughout the 5 weeks, while Friday, Saturday, Sunday, and Monday levels of activity vary greatly throughout the 5 weeks. 

#### Plot activity data by minute and identified by day of week
 
The following code chunk creates a plot that displays the total activity count per minute of the day for each day of the 5 weeks. This results in 35 lines, each color coded for the specific day of the week. The graph is not very readable, but is what I believe the question requests. 
 
```{r activity_plot_ng, message = FALSE}
accel %>% 
  group_by(day, week) %>% 
  ggplot(aes(x = time_minute, y = activity_count, color = day)) +
  geom_line() +
  labs(
    title = "Accelerometer Data by Day",
    x = "Minute of the day (0 - 1440)",
    y = "Activity",
    caption = "This graph has 35 lines each representing a unique day within the 5 weeks of data records.
    The individual days of the week are color coded, with each day of the week having 5 lines."
  )
```

Identfied trends include:

* Minimal activity in the beginning and end points of the graph (presumably early morning and night). 
* Friday has greater activity later in the day. 
* Wednesday has an individual peak in the later middle of the day.

The following code chunk is a different graph that I believe is more readable. It uses the geom_smooth to create intervals and allow clear identification of the day.

```{r activity_plot_nice, message=FALSE, warning=FALSE}
accel %>% 
  group_by(day, week) %>% 
  ggplot(aes(x = time_minute, y = activity_count, color = day)) +
  geom_smooth() +
  labs(
    title = "Accelerometer Data by Day",
    x = "Minute of the day (0 - 1440)",
    y = "Activity",
    caption = "This graph has 7 lines each representing the variation within the 5 weeks of data records."
  )
```

Identfied trends include:

* Minimal activity in the beginning and end points of the graph (presumably early morning and night). 
* Friday has greater activity later in the day. 
* Sunday has a peak in the middle of the day
* Most days of the week mantain similar levels throughout the course of each individual day, with some variation.

## Problem 3

Load in the dataset and tidy. 

```{r load_noaa}
library(p8105.datasets)
data("ny_noaa")
```

#### Dataset Description

The dataset **ny_noaa** was pulled from the NOAA National Climatic Data Center on August 15, 2017. It includes information from all weather stations in New York state ranging from January 1, 1981 to December 31, 2010. 

Weather stations are identified by the variable _id_ and the date is recorded in the variable _date_. Informational variables include _pcrp_ (precipitation), _snow_ (snowfall), _snwd_ (snowdepth), _tmax_ (max temp), and _tmin_ (min temp).

Overall, the dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Each column represents an individual date per weather station.

#### Problem 3 Questions

##### Data cleaning 

The data cleaning step includes creating separate variables for year, month, and day through the separate() function. Temperature variables are mutated from character to numeric variables.

The temperature variable is then reformatted to display temperature in degrees celsius, rather than tenths of a degree celsius. Precipitation is also reformatted to display rainfall in mm, rather than tenths of a mm. Snowfall and snowdepth are already displayed in mm. 

```{r clean_noaa}
ny_noaa_df = 
ny_noaa %>% 
  separate(date, into = c("year", "month"), sep = 5) %>% 
  separate(year, into = c("year", "x"), sep = 4) %>% 
  separate(month, into = c("month", "day"), sep = 2) %>% 
  separate(day, into = c("y", "day"), sep = 1) %>% 
  select(-x, -y) %>% 
  #note: when separating date into different variables in a tidier way, r crashes,
  #most likely due to the size of the dataset. Using brief individual steps 
  #allowed it to process fully on my computer. 
  mutate(tmin = as.numeric(tmin),
         tmax = as.numeric(tmax),
         month = as.numeric(month)
         ) %>% 
  mutate(
    tmin = tmin / 10,
    tmax = tmax / 10, 
    prcp = prcp / 10
    )
```

The following code chunk displays the most common snowfall values in a tibble.

```{r}
ny_noaa_df %>% 
  count(snow) %>% 
  arrange(desc(n))
```
    
For snowfall, the most commonly observed value is 0. This is most likely due to the fact that, the majority of the year, it is not snowing in NYS.
    
##### Two-panel plot showing the average max temperature in January and in July in each station across years

This plot is created by grouping the relevant variables (id, month, year) and then summarizing the tmax into a new variable to allow examination. The plot is then separated by month, using the facet_grid function, to display January and July side by side. Year is plotted along the x axis, from 1981 to 2010, with each colored point representing a unique station in New York State. 

```{r month_plot, message=FALSE, warning=FALSE}
ny_noaa_df %>% 
  filter(month == c(01, 07)) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) + 
  geom_point() +
  geom_line() +
  theme(legend.position = "none") +
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Average Max Temperature (Jan & July)",
    x = "Year",
    y = "Max Temp (C)",
    caption = "Each colored point represents a unique station in NYS."
  )
  
  # Note: legend presents so large that it does not allow view of the plot 
  #(on my screen at least) so it has been removed. 

```

The plot demonstrates that, in January and July, there is mild fluctuation between the yearly reports of max temperature in the weather stations. Further, January consistently reports cooler temperatures throughout this time frame as compared to July. July does have some weather stations that are distanced from the cluster, as seen in 1988 and 2004, but this could represent geographic differences. 

##### Two-panel plot: Temperature and Snowfall

The following code chunk creates two separate plots and displays them side by side through the use of the patchwork function. The first plot code creates a hex plot displaying the comparison between aggregate min and max temperatures. The second plot code creates a ridge plot displayiing the snow distribution by year, filtering by values that are greater than 0 but less than 100 mm. 

```{r two_panel_plot, message = FALSE}
temp = 
  ny_noaa_df %>% 
  drop_na(tmax, tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() +
  labs(
    title = "Temperature Comparison",
    x = "Max Temperature (C)",
    y = "Min Temperature (C)"
  )

snow_plot = 
  ny_noaa_df %>% 
  filter(snow > 0) %>% 
  filter(snow < 100) %>% 
  ggplot(aes(x = snow, y = as.factor(year))) +
  geom_density_ridges() +
  labs(
    title = "Distribution of snow by year",
    x = "Distribution of snow (mm)",
    y = "Year"
  )

temp + snow_plot

```

The first plot, _temp_, demonstrates a wide variety of miin and max temperatures, with a strong concentration of min and max temperatures around 20 (C) and 25 (C) respectively. This represents expected temperatures for most of the year in New York State. A second point of interest lies at min and max temperature of 0 (C) and 5 (C) respectively, which would correspond to the majority of winter months experienced in NYS. 

The second plot, _snow_plot_, displays a steady level of snow distribution in the years of 1981 to 2010. The overall peaks tend to decrease as times go on, but still occur around the same quantity of snow (10, 30, 50, and 75 mm of snow distribution, roughly).







